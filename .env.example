# Example Configuration for OpenPilot

# This file shows all available configuration options
# Copy this to .env in the root directory and customize

# ============================================
# AI Provider Settings
# ============================================

# Provider: ollama, openai, grok, together, huggingface, custom
AI_PROVIDER=ollama

# Model name (depends on provider)
# Ollama: codellama, llama2, mistral, etc.
# OpenAI: gpt-4, gpt-3.5-turbo, etc.
# Grok: grok-1, etc.
AI_MODEL=codellama

# API URL (optional, uses default if not specified)
# Ollama default: http://localhost:11434
# OpenAI default: https://api.openai.com/v1
# AI_API_URL=http://localhost:11434

# API Key (required for cloud providers)
# Keep this secret! Never commit to git
# OPENAI_API_KEY=sk-...
# GROK_API_KEY=xai-...
# TOGETHER_API_KEY=...
# HUGGINGFACE_API_KEY=hf_...

# ============================================
# AI Parameters
# ============================================

# Temperature (0.0 - 2.0)
# Lower = more focused/deterministic
# Higher = more creative/random
AI_TEMPERATURE=0.7

# Max tokens in response
AI_MAX_TOKENS=2048

# Top P (0.0 - 1.0) - nucleus sampling
AI_TOP_P=0.9

# Frequency penalty (-2.0 - 2.0)
AI_FREQUENCY_PENALTY=0

# Presence penalty (-2.0 - 2.0)
AI_PRESENCE_PENALTY=0

# ============================================
# Application Settings
# ============================================

# Offline mode (true/false)
# When true, only local providers work
OFFLINE_MODE=false

# Auto-complete enabled (true/false)
AUTO_COMPLETE=true

# Context lines to include around selection
CONTEXT_LINES=10

# Debug mode (true/false)
DEBUG=false

# Log level: error, warn, info, debug
LOG_LEVEL=info

# ============================================
# Repository Analysis
# ============================================

# Max file size to analyze (in bytes)
MAX_FILE_SIZE=1048576

# Exclude patterns (comma-separated)
EXCLUDE_PATTERNS=node_modules,.git,dist,build,out

# Include patterns (comma-separated)
# INCLUDE_PATTERNS=**/*.ts,**/*.js,**/*.py

# Enable automatic repository analysis on startup
AUTO_ANALYZE_REPO=true

# ============================================
# Performance Settings
# ============================================

# Cache size (in MB)
CACHE_SIZE=100

# Enable streaming responses
ENABLE_STREAMING=true

# Debounce delay for completions (ms)
COMPLETION_DEBOUNCE=300

# Request timeout (ms)
REQUEST_TIMEOUT=60000

# ============================================
# Privacy Settings
# ============================================

# Telemetry disabled by default
TELEMETRY_ENABLED=false

# Store chat history locally
STORE_HISTORY=true

# Encrypt stored data
ENCRYPT_DATA=true

# ============================================
# Advanced Settings
# ============================================

# Custom system prompt
# SYSTEM_PROMPT=You are an expert coding assistant...

# Max chat history to send with requests
MAX_HISTORY_MESSAGES=10

# Enable bias detection
ENABLE_BIAS_DETECTION=false

# Enable performance metrics
ENABLE_METRICS=false

# ============================================
# Backend Server (Optional)
# ============================================

# Enable backend server
ENABLE_BACKEND=false

# Backend URL
# BACKEND_URL=http://localhost:8000

# Backend API key
# BACKEND_API_KEY=

# ============================================
# Development Settings
# ============================================

# Hot reload enabled
HOT_RELOAD=true

# Source maps enabled
SOURCE_MAPS=true

# Strict mode
STRICT_MODE=true

# ============================================
# Platform-Specific Settings
# ============================================

# Desktop
# ELECTRON_START_URL=http://localhost:3000

# Web
# REACT_APP_API_URL=http://localhost:8000
# REACT_APP_WS_URL=ws://localhost:8000

# Mobile
# MOBILE_API_URL=http://localhost:8000

# ============================================
# Example Configurations
# ============================================

# Local Development with Ollama
# AI_PROVIDER=ollama
# AI_MODEL=codellama
# OFFLINE_MODE=true

# OpenAI GPT-4
# AI_PROVIDER=openai
# AI_MODEL=gpt-4
# OPENAI_API_KEY=sk-...

# Grok (xAI)
# AI_PROVIDER=grok
# AI_MODEL=grok-1
# GROK_API_KEY=xai-...

# Together AI
# AI_PROVIDER=together
# AI_MODEL=codellama/CodeLlama-34b-Instruct-hf
# TOGETHER_API_KEY=...
# AI_API_URL=https://api.together.xyz/v1

# Hugging Face
# AI_PROVIDER=huggingface
# AI_MODEL=bigcode/starcoder
# HUGGINGFACE_API_KEY=hf_...
# AI_API_URL=https://api-inference.huggingface.co/models

# Custom OpenAI-compatible API
# AI_PROVIDER=custom
# AI_MODEL=your-model
# AI_API_URL=https://your-api.com/v1
# AI_API_KEY=your-key

# ============================================
# Notes
# ============================================

# - Lines starting with # are comments
# - Remove # to enable a setting
# - Never commit API keys to version control
# - Use .env.local for local overrides (gitignored)
# - Environment variables override these settings
